{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load librabies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronsteiner/Documents/GitHub/BugPlusEngine/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import torch.optim as optim\n",
    "import dgl\n",
    "import random\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import itertools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graph and the deleted edge information from the file\n",
    "with open(\"../../graphs_and_deleted_edges.pickle\", \"rb\") as f:\n",
    "    all_graphs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=3, num_edges=12,\n",
      "      ndata_schemes={'node_id': Scheme(shape=(), dtype=torch.int64), 'Up': Scheme(shape=(1,), dtype=torch.float32), 'Down': Scheme(shape=(1,), dtype=torch.float32), 'Right': Scheme(shape=(1,), dtype=torch.float32), 'Left': Scheme(shape=(1,), dtype=torch.float32), 'Out': Scheme(shape=(1,), dtype=torch.float32), 'In': Scheme(shape=(1,), dtype=torch.float32)}\n",
      "      edata_schemes={'src_port': Scheme(shape=(), dtype=torch.int64), 'dst_port': Scheme(shape=(), dtype=torch.int64), 'edge_type': Scheme(shape=(), dtype=torch.int64)})\n"
     ]
    }
   ],
   "source": [
    "print(all_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split edge set for training and testing\n",
    "g = all_graphs\n",
    "u, v = g.edges()\n",
    "\n",
    "eids = np.arange(g.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "test_size = 1\n",
    "train_size = g.number_of_edges() - test_size\n",
    "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
    "\n",
    "# Find all negative edges and split them for training and testing\n",
    "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
    "adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
    "neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "neg_eids = np.random.choice(len(neg_u), g.number_of_edges())\n",
    "test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_g = dgl.remove_edges(g, eids[:test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer GraphSAGE model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())\n",
    "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
    "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "\n",
    "class DotPredictor(nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Compute a new edge feature named 'score' by a dot-product between the\n",
    "            # source node feature 'h' and destination node feature 'h'.\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "            return g.edata['score'][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphSAGE(1, 16)\n",
    "# You can replace DotPredictor with MLPPredictor.\n",
    "#pred = MLPPredictor(16)\n",
    "pred = DotPredictor()\n",
    "\n",
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "    return roc_auc_score(labels, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 3398269.75\n",
      "In epoch 5, loss: 698297.9375\n",
      "In epoch 10, loss: 139072.03125\n",
      "In epoch 15, loss: 72298.734375\n",
      "In epoch 20, loss: 26253.185546875\n",
      "In epoch 25, loss: 42183.68359375\n",
      "In epoch 30, loss: 11491.306640625\n",
      "In epoch 35, loss: 14485.5341796875\n",
      "In epoch 40, loss: 4355.78955078125\n",
      "In epoch 45, loss: 4372.287109375\n",
      "In epoch 50, loss: 17022.5546875\n",
      "In epoch 55, loss: 10105.083984375\n",
      "In epoch 60, loss: 17702.01953125\n",
      "In epoch 65, loss: 9411.9287109375\n",
      "In epoch 70, loss: 13864.666015625\n",
      "In epoch 75, loss: 9938.47265625\n",
      "In epoch 80, loss: 4470.14404296875\n",
      "In epoch 85, loss: 7918.541015625\n",
      "In epoch 90, loss: 7635.53076171875\n",
      "In epoch 95, loss: 4985.84228515625\n",
      "In epoch 100, loss: 15267.671875\n",
      "In epoch 105, loss: 5373.400390625\n",
      "In epoch 110, loss: 4500.8779296875\n",
      "In epoch 115, loss: 4969.04833984375\n",
      "In epoch 120, loss: 6060.79638671875\n",
      "In epoch 125, loss: 1177.4405517578125\n",
      "In epoch 130, loss: 1996.7987060546875\n",
      "In epoch 135, loss: 10155.92578125\n",
      "In epoch 140, loss: 10220.9921875\n",
      "In epoch 145, loss: 9074.68359375\n",
      "In epoch 150, loss: 3046.868896484375\n",
      "In epoch 155, loss: 7041.65771484375\n",
      "In epoch 160, loss: 9519.2724609375\n",
      "In epoch 165, loss: 4901.00830078125\n",
      "In epoch 170, loss: 3119.549072265625\n",
      "In epoch 175, loss: 8002.80908203125\n",
      "In epoch 180, loss: 5468.56689453125\n",
      "In epoch 185, loss: 7949.69873046875\n",
      "In epoch 190, loss: 5240.42041015625\n",
      "In epoch 195, loss: 9386.9521484375\n",
      "In epoch 200, loss: 12438.8154296875\n",
      "In epoch 205, loss: 6147.8984375\n",
      "In epoch 210, loss: 1446.5079345703125\n",
      "In epoch 215, loss: 16871.142578125\n",
      "In epoch 220, loss: 5681.82470703125\n",
      "In epoch 225, loss: 6558.025390625\n",
      "In epoch 230, loss: 4022.29541015625\n",
      "In epoch 235, loss: 2017.63427734375\n",
      "In epoch 240, loss: 8510.3037109375\n",
      "In epoch 245, loss: 12925.5126953125\n",
      "In epoch 250, loss: 16138.1962890625\n",
      "In epoch 255, loss: 16697.2734375\n",
      "In epoch 260, loss: 9017.693359375\n",
      "In epoch 265, loss: 10123.8935546875\n",
      "In epoch 270, loss: 10356.5546875\n",
      "In epoch 275, loss: 6073.439453125\n",
      "In epoch 280, loss: 21277.9296875\n",
      "In epoch 285, loss: 9000.7421875\n",
      "In epoch 290, loss: 5252.10009765625\n",
      "In epoch 295, loss: 5892.11669921875\n",
      "In epoch 300, loss: 1970.40869140625\n",
      "In epoch 305, loss: 6103.21826171875\n",
      "In epoch 310, loss: 11515.90625\n",
      "In epoch 315, loss: 6253.845703125\n",
      "In epoch 320, loss: 10197.814453125\n",
      "In epoch 325, loss: 7858.708984375\n",
      "In epoch 330, loss: 6362.08544921875\n",
      "In epoch 335, loss: 8700.5009765625\n",
      "In epoch 340, loss: 9473.7587890625\n",
      "In epoch 345, loss: 3245.370849609375\n",
      "In epoch 350, loss: 12475.6474609375\n",
      "In epoch 355, loss: 4989.21044921875\n",
      "In epoch 360, loss: 5721.4677734375\n",
      "In epoch 365, loss: 5659.2001953125\n",
      "In epoch 370, loss: 1194.5277099609375\n",
      "In epoch 375, loss: 6275.79833984375\n",
      "In epoch 380, loss: 4676.04833984375\n",
      "In epoch 385, loss: 6981.8515625\n",
      "In epoch 390, loss: 5184.25634765625\n",
      "In epoch 395, loss: 10304.6943359375\n",
      "In epoch 400, loss: 8693.71484375\n",
      "In epoch 405, loss: 6753.86669921875\n",
      "In epoch 410, loss: 5878.43701171875\n",
      "In epoch 415, loss: 7041.77978515625\n",
      "In epoch 420, loss: 6220.27197265625\n",
      "In epoch 425, loss: 6411.93701171875\n",
      "In epoch 430, loss: 6920.27197265625\n",
      "In epoch 435, loss: 6679.255859375\n",
      "In epoch 440, loss: 6459.40185546875\n",
      "In epoch 445, loss: 11738.9521484375\n",
      "In epoch 450, loss: 15364.3720703125\n",
      "In epoch 455, loss: 15691.9248046875\n",
      "In epoch 460, loss: 5517.8505859375\n",
      "In epoch 465, loss: 7276.193359375\n",
      "In epoch 470, loss: 12826.8525390625\n",
      "In epoch 475, loss: 10040.6181640625\n",
      "In epoch 480, loss: 4749.93310546875\n",
      "In epoch 485, loss: 4273.62353515625\n",
      "In epoch 490, loss: 5847.666015625\n",
      "In epoch 495, loss: 5846.43701171875\n",
      "In epoch 500, loss: 4908.708984375\n",
      "In epoch 505, loss: 2837.60009765625\n",
      "In epoch 510, loss: 1640.7315673828125\n",
      "In epoch 515, loss: 2005.76806640625\n",
      "In epoch 520, loss: 2435.857421875\n",
      "In epoch 525, loss: 2249.499755859375\n",
      "In epoch 530, loss: 2795.057373046875\n",
      "In epoch 535, loss: 2970.522705078125\n",
      "In epoch 540, loss: 3511.936767578125\n",
      "In epoch 545, loss: 2195.1376953125\n",
      "In epoch 550, loss: 5722.32861328125\n",
      "In epoch 555, loss: 8040.9189453125\n",
      "In epoch 560, loss: 4217.9990234375\n",
      "In epoch 565, loss: 2662.334228515625\n",
      "In epoch 570, loss: 4991.8583984375\n",
      "In epoch 575, loss: 7179.68603515625\n",
      "In epoch 580, loss: 2237.352294921875\n",
      "In epoch 585, loss: 1511.2012939453125\n",
      "In epoch 590, loss: 4624.21240234375\n",
      "In epoch 595, loss: 1191.3875732421875\n",
      "In epoch 600, loss: 5328.6142578125\n",
      "In epoch 605, loss: 4485.5908203125\n",
      "In epoch 610, loss: 4128.11669921875\n",
      "In epoch 615, loss: 1828.21435546875\n",
      "In epoch 620, loss: 4929.5732421875\n",
      "In epoch 625, loss: 3057.5751953125\n",
      "In epoch 630, loss: 2925.39599609375\n",
      "In epoch 635, loss: 1722.6697998046875\n",
      "In epoch 640, loss: 3209.134521484375\n",
      "In epoch 645, loss: 2951.775146484375\n",
      "In epoch 650, loss: 3133.874267578125\n",
      "In epoch 655, loss: 1484.0789794921875\n",
      "In epoch 660, loss: 4931.291015625\n",
      "In epoch 665, loss: 4357.41748046875\n",
      "In epoch 670, loss: 4967.3603515625\n",
      "In epoch 675, loss: 3477.168701171875\n",
      "In epoch 680, loss: 3298.83837890625\n",
      "In epoch 685, loss: 684.8803100585938\n",
      "In epoch 690, loss: 3794.046630859375\n",
      "In epoch 695, loss: 7022.45947265625\n",
      "In epoch 700, loss: 5440.71533203125\n",
      "In epoch 705, loss: 1933.96142578125\n",
      "In epoch 710, loss: 3638.65478515625\n",
      "In epoch 715, loss: 2758.315185546875\n",
      "In epoch 720, loss: 2803.929443359375\n",
      "In epoch 725, loss: 921.691650390625\n",
      "In epoch 730, loss: 1087.717529296875\n",
      "In epoch 735, loss: 1115.96826171875\n",
      "In epoch 740, loss: 4786.2880859375\n",
      "In epoch 745, loss: 4108.326171875\n",
      "In epoch 750, loss: 3341.113037109375\n",
      "In epoch 755, loss: 3799.665771484375\n",
      "In epoch 760, loss: 5093.892578125\n",
      "In epoch 765, loss: 3162.8447265625\n",
      "In epoch 770, loss: 2090.510498046875\n",
      "In epoch 775, loss: 2898.392333984375\n",
      "In epoch 780, loss: 2616.639892578125\n",
      "In epoch 785, loss: 1142.908935546875\n",
      "In epoch 790, loss: 940.306396484375\n",
      "In epoch 795, loss: 2537.507568359375\n",
      "In epoch 800, loss: 3131.703125\n",
      "In epoch 805, loss: 4358.91748046875\n",
      "In epoch 810, loss: 2767.060791015625\n",
      "In epoch 815, loss: 2088.424072265625\n",
      "In epoch 820, loss: 1777.4949951171875\n",
      "In epoch 825, loss: 1766.89892578125\n",
      "In epoch 830, loss: 3002.133544921875\n",
      "In epoch 835, loss: 1397.5072021484375\n",
      "In epoch 840, loss: 1655.2164306640625\n",
      "In epoch 845, loss: 1762.0003662109375\n",
      "In epoch 850, loss: 1120.5166015625\n",
      "In epoch 855, loss: 2004.1463623046875\n",
      "In epoch 860, loss: 2750.296630859375\n",
      "In epoch 865, loss: 541.9960327148438\n",
      "In epoch 870, loss: 2462.464599609375\n",
      "In epoch 875, loss: 2248.683349609375\n",
      "In epoch 880, loss: 1541.18115234375\n",
      "In epoch 885, loss: 2338.46435546875\n",
      "In epoch 890, loss: 1239.9080810546875\n",
      "In epoch 895, loss: 1282.6048583984375\n",
      "In epoch 900, loss: 885.0151977539062\n",
      "In epoch 905, loss: 443.72918701171875\n",
      "In epoch 910, loss: 822.9451293945312\n",
      "In epoch 915, loss: 1612.1968994140625\n",
      "In epoch 920, loss: 1341.785888671875\n",
      "In epoch 925, loss: 1476.700439453125\n",
      "In epoch 930, loss: 649.906982421875\n",
      "In epoch 935, loss: 4799.3017578125\n",
      "In epoch 940, loss: 3495.48583984375\n",
      "In epoch 945, loss: 3475.089599609375\n",
      "In epoch 950, loss: 1968.8760986328125\n",
      "In epoch 955, loss: 1468.30029296875\n",
      "In epoch 960, loss: 1545.619873046875\n",
      "In epoch 965, loss: 1519.108154296875\n",
      "In epoch 970, loss: 940.1339721679688\n",
      "In epoch 975, loss: 634.3677368164062\n",
      "In epoch 980, loss: 1329.1832275390625\n",
      "In epoch 985, loss: 1218.9366455078125\n",
      "In epoch 990, loss: 2092.8701171875\n",
      "In epoch 995, loss: 2551.24169921875\n",
      "In epoch 1000, loss: 2014.83349609375\n",
      "In epoch 1005, loss: 1305.8050537109375\n",
      "In epoch 1010, loss: 697.0404663085938\n",
      "In epoch 1015, loss: 364.0135803222656\n",
      "In epoch 1020, loss: 539.310791015625\n",
      "In epoch 1025, loss: 2238.9970703125\n",
      "In epoch 1030, loss: 671.2056274414062\n",
      "In epoch 1035, loss: 2048.677978515625\n",
      "In epoch 1040, loss: 2831.566162109375\n",
      "In epoch 1045, loss: 1878.52783203125\n",
      "In epoch 1050, loss: 1758.0018310546875\n",
      "In epoch 1055, loss: 512.4321899414062\n",
      "In epoch 1060, loss: 811.0274047851562\n",
      "In epoch 1065, loss: 2775.353271484375\n",
      "In epoch 1070, loss: 2422.919677734375\n",
      "In epoch 1075, loss: 1082.309326171875\n",
      "In epoch 1080, loss: 375.1701354980469\n",
      "In epoch 1085, loss: 199.6239776611328\n",
      "In epoch 1090, loss: 863.3638916015625\n",
      "In epoch 1095, loss: 883.3488159179688\n",
      "In epoch 1100, loss: 542.5997924804688\n",
      "In epoch 1105, loss: 264.20001220703125\n",
      "In epoch 1110, loss: 771.4640502929688\n",
      "In epoch 1115, loss: 675.338623046875\n",
      "In epoch 1120, loss: 1623.014892578125\n",
      "In epoch 1125, loss: 833.8729858398438\n",
      "In epoch 1130, loss: 755.6157836914062\n",
      "In epoch 1135, loss: 2879.955322265625\n",
      "In epoch 1140, loss: 2991.3544921875\n",
      "In epoch 1145, loss: 1575.45068359375\n",
      "In epoch 1150, loss: 1155.14013671875\n",
      "In epoch 1155, loss: 1559.6136474609375\n",
      "In epoch 1160, loss: 902.3524780273438\n",
      "In epoch 1165, loss: 641.8544921875\n",
      "In epoch 1170, loss: 1068.6453857421875\n",
      "In epoch 1175, loss: 643.4204711914062\n",
      "In epoch 1180, loss: 505.0176696777344\n",
      "In epoch 1185, loss: 1579.1502685546875\n",
      "In epoch 1190, loss: 1243.8397216796875\n",
      "In epoch 1195, loss: 1132.89404296875\n",
      "In epoch 1200, loss: 954.0277099609375\n",
      "In epoch 1205, loss: 1113.970947265625\n",
      "In epoch 1210, loss: 2075.115478515625\n",
      "In epoch 1215, loss: 1743.3421630859375\n",
      "In epoch 1220, loss: 1107.6400146484375\n",
      "In epoch 1225, loss: 2129.976318359375\n",
      "In epoch 1230, loss: 1598.9356689453125\n",
      "In epoch 1235, loss: 1561.0528564453125\n",
      "In epoch 1240, loss: 901.4349975585938\n",
      "In epoch 1245, loss: 1054.7947998046875\n",
      "In epoch 1250, loss: 1071.3663330078125\n",
      "In epoch 1255, loss: 811.6943359375\n",
      "In epoch 1260, loss: 1536.7606201171875\n",
      "In epoch 1265, loss: 1138.2626953125\n",
      "In epoch 1270, loss: 2372.8486328125\n",
      "In epoch 1275, loss: 1129.092529296875\n",
      "In epoch 1280, loss: 729.8071899414062\n",
      "In epoch 1285, loss: 666.2836303710938\n",
      "In epoch 1290, loss: 642.6382446289062\n",
      "In epoch 1295, loss: 570.3796997070312\n",
      "In epoch 1300, loss: 794.839111328125\n",
      "In epoch 1305, loss: 484.7850341796875\n",
      "In epoch 1310, loss: 1125.2066650390625\n",
      "In epoch 1315, loss: 855.0161743164062\n",
      "In epoch 1320, loss: 1995.89697265625\n",
      "In epoch 1325, loss: 1794.1942138671875\n",
      "In epoch 1330, loss: 660.2779541015625\n",
      "In epoch 1335, loss: 1431.8309326171875\n",
      "In epoch 1340, loss: 1726.7769775390625\n",
      "In epoch 1345, loss: 1030.5625\n",
      "In epoch 1350, loss: 1179.2728271484375\n",
      "In epoch 1355, loss: 890.7329711914062\n",
      "In epoch 1360, loss: 125.66405487060547\n",
      "In epoch 1365, loss: 808.371337890625\n",
      "In epoch 1370, loss: 919.4720458984375\n",
      "In epoch 1375, loss: 827.5978393554688\n",
      "In epoch 1380, loss: 901.3259887695312\n",
      "In epoch 1385, loss: 1431.9779052734375\n",
      "In epoch 1390, loss: 1834.3355712890625\n",
      "In epoch 1395, loss: 1133.348388671875\n",
      "In epoch 1400, loss: 1370.7384033203125\n",
      "In epoch 1405, loss: 1097.552978515625\n",
      "In epoch 1410, loss: 1158.3861083984375\n",
      "In epoch 1415, loss: 842.189453125\n",
      "In epoch 1420, loss: 482.7549743652344\n",
      "In epoch 1425, loss: 307.24407958984375\n",
      "In epoch 1430, loss: 398.07000732421875\n",
      "In epoch 1435, loss: 615.00732421875\n",
      "In epoch 1440, loss: 436.56854248046875\n",
      "In epoch 1445, loss: 1230.3446044921875\n",
      "In epoch 1450, loss: 410.8301086425781\n",
      "In epoch 1455, loss: 1169.2088623046875\n",
      "In epoch 1460, loss: 831.530517578125\n",
      "In epoch 1465, loss: 1115.1468505859375\n",
      "In epoch 1470, loss: 1249.2401123046875\n",
      "In epoch 1475, loss: 1661.95166015625\n",
      "In epoch 1480, loss: 1751.2491455078125\n",
      "In epoch 1485, loss: 1464.8775634765625\n",
      "In epoch 1490, loss: 875.7684936523438\n",
      "In epoch 1495, loss: 1781.7183837890625\n",
      "In epoch 1500, loss: 1771.0489501953125\n",
      "In epoch 1505, loss: 582.8109130859375\n",
      "In epoch 1510, loss: 1214.4735107421875\n",
      "In epoch 1515, loss: 335.9106750488281\n",
      "In epoch 1520, loss: 612.7890625\n",
      "In epoch 1525, loss: 988.9544677734375\n",
      "In epoch 1530, loss: 941.9603881835938\n",
      "In epoch 1535, loss: 801.0967407226562\n",
      "In epoch 1540, loss: 1360.066162109375\n",
      "In epoch 1545, loss: 415.24151611328125\n",
      "In epoch 1550, loss: 388.1690368652344\n",
      "In epoch 1555, loss: 1206.0645751953125\n",
      "In epoch 1560, loss: 1056.7054443359375\n",
      "In epoch 1565, loss: 1262.6949462890625\n",
      "In epoch 1570, loss: 968.4713134765625\n",
      "In epoch 1575, loss: 823.8375244140625\n",
      "In epoch 1580, loss: 629.125732421875\n",
      "In epoch 1585, loss: 826.876953125\n",
      "In epoch 1590, loss: 818.5161743164062\n",
      "In epoch 1595, loss: 777.6380615234375\n",
      "In epoch 1600, loss: 739.9886474609375\n",
      "In epoch 1605, loss: 376.5672607421875\n",
      "In epoch 1610, loss: 749.0338134765625\n",
      "In epoch 1615, loss: 631.299560546875\n",
      "In epoch 1620, loss: 1301.17919921875\n",
      "In epoch 1625, loss: 1438.4132080078125\n",
      "In epoch 1630, loss: 1979.7301025390625\n",
      "In epoch 1635, loss: 1728.3956298828125\n",
      "In epoch 1640, loss: 823.6704711914062\n",
      "In epoch 1645, loss: 1849.311767578125\n",
      "In epoch 1650, loss: 1473.6302490234375\n",
      "In epoch 1655, loss: 704.7853393554688\n",
      "In epoch 1660, loss: 1062.2047119140625\n",
      "In epoch 1665, loss: 500.1420593261719\n",
      "In epoch 1670, loss: 260.7127685546875\n",
      "In epoch 1675, loss: 215.86074829101562\n",
      "In epoch 1680, loss: 598.281982421875\n",
      "In epoch 1685, loss: 580.2080688476562\n",
      "In epoch 1690, loss: 710.4605102539062\n",
      "In epoch 1695, loss: 805.2094116210938\n",
      "In epoch 1700, loss: 1087.2069091796875\n",
      "In epoch 1705, loss: 533.6260375976562\n",
      "In epoch 1710, loss: 152.26136779785156\n",
      "In epoch 1715, loss: 294.8841552734375\n",
      "In epoch 1720, loss: 628.8789672851562\n",
      "In epoch 1725, loss: 633.3978271484375\n",
      "In epoch 1730, loss: 433.3414001464844\n",
      "In epoch 1735, loss: 659.580078125\n",
      "In epoch 1740, loss: 516.7587280273438\n",
      "In epoch 1745, loss: 302.0892639160156\n",
      "In epoch 1750, loss: 670.732421875\n",
      "In epoch 1755, loss: 432.2846374511719\n",
      "In epoch 1760, loss: 747.2188110351562\n",
      "In epoch 1765, loss: 1053.8980712890625\n",
      "In epoch 1770, loss: 350.8450622558594\n",
      "In epoch 1775, loss: 166.39271545410156\n",
      "In epoch 1780, loss: 459.1379699707031\n",
      "In epoch 1785, loss: 1047.7032470703125\n",
      "In epoch 1790, loss: 940.2348022460938\n",
      "In epoch 1795, loss: 1148.9058837890625\n",
      "In epoch 1800, loss: 699.5806274414062\n",
      "In epoch 1805, loss: 849.8250122070312\n",
      "In epoch 1810, loss: 799.0540771484375\n",
      "In epoch 1815, loss: 607.8892822265625\n",
      "In epoch 1820, loss: 900.7109375\n",
      "In epoch 1825, loss: 556.8667602539062\n",
      "In epoch 1830, loss: 170.84117126464844\n",
      "In epoch 1835, loss: 141.8707733154297\n",
      "In epoch 1840, loss: 483.2765808105469\n",
      "In epoch 1845, loss: 729.6431274414062\n",
      "In epoch 1850, loss: 228.83934020996094\n",
      "In epoch 1855, loss: 298.1265869140625\n",
      "In epoch 1860, loss: 355.11865234375\n",
      "In epoch 1865, loss: 752.5772094726562\n",
      "In epoch 1870, loss: 794.904541015625\n",
      "In epoch 1875, loss: 541.3154296875\n",
      "In epoch 1880, loss: 1073.5472412109375\n",
      "In epoch 1885, loss: 1121.708740234375\n",
      "In epoch 1890, loss: 882.3226318359375\n",
      "In epoch 1895, loss: 644.7807006835938\n",
      "In epoch 1900, loss: 634.6688842773438\n",
      "In epoch 1905, loss: 669.2543334960938\n",
      "In epoch 1910, loss: 352.0569152832031\n",
      "In epoch 1915, loss: 591.68798828125\n",
      "In epoch 1920, loss: 364.52520751953125\n",
      "In epoch 1925, loss: 778.926025390625\n",
      "In epoch 1930, loss: 734.461181640625\n",
      "In epoch 1935, loss: 929.9922485351562\n",
      "In epoch 1940, loss: 1021.579833984375\n",
      "In epoch 1945, loss: 160.94989013671875\n",
      "In epoch 1950, loss: 293.29547119140625\n",
      "In epoch 1955, loss: 486.66650390625\n",
      "In epoch 1960, loss: 983.085205078125\n",
      "In epoch 1965, loss: 411.91717529296875\n",
      "In epoch 1970, loss: 323.6521911621094\n",
      "In epoch 1975, loss: 1170.2362060546875\n",
      "In epoch 1980, loss: 478.1993103027344\n",
      "In epoch 1985, loss: 434.92254638671875\n",
      "In epoch 1990, loss: 534.2977905273438\n",
      "In epoch 1995, loss: 405.65185546875\n",
      "AUC 0.5\n"
     ]
    }
   ],
   "source": [
    "# ----------- 3. set up loss and optimizer -------------- #\n",
    "# in this case, loss will in training loop\n",
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.01)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "for e in range(2_000):\n",
    "    # forward\n",
    "    h = model(train_g, train_g.ndata['Up'])\n",
    "    pos_score = pred(train_pos_g, h)\n",
    "    neg_score = pred(train_neg_g, h)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print('In epoch {}, loss: {}'.format(e, loss))\n",
    "\n",
    "# ----------- 5. check results ------------------------ #\n",
    "from sklearn.metrics import roc_auc_score\n",
    "with torch.no_grad():\n",
    "    pos_score = pred(test_pos_g, h)\n",
    "    neg_score = pred(test_neg_g, h)\n",
    "    print('AUC', compute_auc(pos_score, neg_score))\n",
    "\n",
    "\n",
    "# Thumbnail credits: Link Prediction with Neo4j, Mark Needham\n",
    "# sphinx_gallery_thumbnail_path = '_static/blitz_4_link_predict.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_port': tensor([3, 2, 4, 4, 0, 0, 5, 3, 2, 1, 1]), 'dst_port': tensor([3, 2, 4, 4, 0, 0, 5, 5, 5, 1, 1]), 'edge_type': tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_g.edata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Make a prediction on the first node pair.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m u, v \u001b[39m=\u001b[39m train_g\u001b[39m.\u001b[39medges()\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEdge features of the first edge:\u001b[39m\u001b[39m'\u001b[39m, train_g\u001b[39m.\u001b[39;49medata[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPrediction score of the first edge:\u001b[39m\u001b[39m'\u001b[39m, pred(train_g, h)[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/GitHub/BugPlusEngine/env/lib/python3.10/site-packages/dgl/view.py:192\u001b[0m, in \u001b[0;36mHeteroEdgeDataView.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n\u001b[1;32m    191\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49m_get_e_repr(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_etid, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_edges)[key]\n",
      "File \u001b[0;32m~/Documents/GitHub/BugPlusEngine/env/lib/python3.10/site-packages/dgl/frame.py:622\u001b[0m, in \u001b[0;36mFrame.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m    610\u001b[0m     \u001b[39m\"\"\"Return the column of the given name.\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \n\u001b[1;32m    612\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[39m        Column data.\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_columns[name]\u001b[39m.\u001b[39mdata\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Make a prediction on the first node pair.\n",
    "u, v = train_g.edges()\n",
    "print('Edge features of the first edge:', train_g.edata[0])\n",
    "print('Prediction score of the first edge:', pred(train_g, h)[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the graph and the deleted edge information in a list\n",
    "graphs = []\n",
    "target = []\n",
    "\n",
    "\n",
    "# Get all edges \n",
    "all_eids = all_graphs.edges()\n",
    "\n",
    "# Convert the tensor to a list for easier iteration\n",
    "all_eids = list(all_eids)\n",
    "\n",
    "# Loop through all edge ids\n",
    "for eid in range(len(all_eids[0])):\n",
    "    # Get the source and destination node id\n",
    "    u = all_eids[0][eid]\n",
    "    v = all_eids[1][eid]\n",
    "    # Append the edge to the deleted edges list\n",
    "    target.append((u, v, all_graphs.edges[u, v].data))\n",
    "    # Remove the edge from the graph\n",
    "    all_graphs = dgl.remove_edges(all_graphs, eid)                     \n",
    "    # Append the graph to the list\n",
    "    graphs.append(all_graphs)\n",
    "    # Add the edge back to the graph\n",
    "    all_graphs = dgl.add_edges(all_graphs, u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor(1),\n",
       "  tensor(0),\n",
       "  {'src_port': tensor([2]), 'dst_port': tensor([2]), 'edge_type': tensor([0])}),\n",
       " (tensor(1),\n",
       "  tensor(0),\n",
       "  {'src_port': tensor([4]), 'dst_port': tensor([4]), 'edge_type': tensor([1])}),\n",
       " (tensor(0),\n",
       "  tensor(2),\n",
       "  {'src_port': tensor([0]), 'dst_port': tensor([0]), 'edge_type': tensor([0])})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer GraphSAGE model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GCN message passing function\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # Compute the node representations by message passing\n",
    "        g.ndata['h'] = inputs\n",
    "        g.update_all(fn.copy_src('h', 'm'), fn.sum('m', 'h'))\n",
    "        h = g.ndata.pop('h')\n",
    "        return self.linear(h)\n",
    "\n",
    "# Define the GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gcn1 = GCNLayer(in_feats, hidden_size)\n",
    "        self.gcn2 = GCNLayer(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = self.gcn1(g, inputs)\n",
    "        h = torch.relu(h)\n",
    "        h = self.gcn2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss function, and optimizer\n",
    "model = GCN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(features, adjacency_matrix)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_graphs = []\n",
    "train_deleted_edges = []\n",
    "val_graphs = []\n",
    "val_deleted_edges = []\n",
    "\n",
    "for graph, deleted_edge in zip(all_graphs, all_deleted_edges):\n",
    "    if random.random() < 0.8:\n",
    "        train_graphs.append(graph)\n",
    "        train_deleted_edges.append(deleted_edge)\n",
    "    else:\n",
    "        val_graphs.append(graph)\n",
    "        val_deleted_edges.append(deleted_edge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'HeteroNodeDataView'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m# Define the adjacency matrices\u001b[39;00m\n\u001b[1;32m     36\u001b[0m adjacency_matrices \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mtensor(nx\u001b[39m.\u001b[39madjacency_matrix(g)\u001b[39m.\u001b[39mtodense()) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m nx_graphs]\n\u001b[0;32m---> 37\u001b[0m node_features \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mtensor(g\u001b[39m.\u001b[39mndata) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m graphs]\n\u001b[1;32m     40\u001b[0m \u001b[39m# Train the GCN model\u001b[39;00m\n\u001b[1;32m     41\u001b[0m model \u001b[39m=\u001b[39m GCN(in_features, hidden_features, out_features)\n",
      "Cell \u001b[0;32mIn[61], line 37\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m# Define the adjacency matrices\u001b[39;00m\n\u001b[1;32m     36\u001b[0m adjacency_matrices \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mtensor(nx\u001b[39m.\u001b[39madjacency_matrix(g)\u001b[39m.\u001b[39mtodense()) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m nx_graphs]\n\u001b[0;32m---> 37\u001b[0m node_features \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39;49mtensor(g\u001b[39m.\u001b[39;49mndata) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m graphs]\n\u001b[1;32m     40\u001b[0m \u001b[39m# Train the GCN model\u001b[39;00m\n\u001b[1;32m     41\u001b[0m model \u001b[39m=\u001b[39m GCN(in_features, hidden_features, out_features)\n",
      "\u001b[0;31mValueError\u001b[0m: could not determine the shape of object type 'HeteroNodeDataView'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define your GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super(GCN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_features, out_features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = self.layers(x)\n",
    "        x = torch.mm(adj, x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# in_features is the number of features of each node\n",
    "in_features = graphs[0].number_of_nodes()\n",
    "# hidden_features is the number of features of the hidden layer\n",
    "hidden_features = 16\n",
    "# out_features is the number of features of the output\n",
    "out_features = 1\n",
    "\n",
    "# Define the adjacency matrices\n",
    "# Convert the DGL heterographs to NetworkX graphs\n",
    "nx_graphs = [g.to_networkx() for g in graphs]\n",
    "\n",
    "# Define the adjacency matrices\n",
    "adjacency_matrices = [torch.tensor(nx.adjacency_matrix(g).todense()) for g in nx_graphs]\n",
    "node_features = [torch.tensor(g.ndata) for g in graphs]\n",
    "\n",
    "\n",
    "# Train the GCN model\n",
    "model = GCN(in_features, hidden_features, out_features)\n",
    "model.train()\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = [model(g, adj) for g, adj in zip(graphs, adjacency_matrices)]\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "    loss = criterion(outputs, target)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = [model(g, adj) for g, adj in zip(graphs, adjacency_matrices)]\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "    loss = criterion(outputs, target)\n",
    "    print(f'Final loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 11\n"
     ]
    }
   ],
   "source": [
    "print(len(train_graphs), len(train_deleted_edges))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "070cdce012dd41a01f1f0ff20239b474e092c7a2b169ddafdace8d691490e16c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
