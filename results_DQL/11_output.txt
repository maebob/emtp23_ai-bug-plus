line 31, vector:
 [8 1 9 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0
 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]
Episode:  0
t:  0
action:  tensor([[69]])
reward:  -10
line 245: next_state:  tensor([[0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.]])
Episode:  1
t:  0
action:  tensor([[26]])
reward:  -10
line 245: next_state:  tensor([[0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.]])
Episode:  2
t:  0
action:  tensor([[11]])
reward:  -10
line 245: next_state:  tensor([[0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.]])
Episode:  3
t:  0
action:  tensor([[35]])
reward:  -10
line 245: next_state:  tensor([[0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,
         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.]])
Episode:  4
t:  0
action:  tensor([[30]])
reward:  -10
line 245: next_state:  tensor([[0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.,
         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.]])
Episode:  5
t:  0
action:  tensor([[36]])
reward:  -10
line 245: next_state:  tensor([[0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.,
         1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.]])
Episode:  6
t:  0
action:  tensor([[23]])
reward:  -10
line 245: next_state:  tensor([[0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
         0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.,
         1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.]])
Episode:  7
t:  0
action:  tensor([[45]])
reward:  0
line 245: next_state:  tensor([[0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
         0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.,
         1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.]])
Episode:  8
t:  0
action:  tensor([[66]])
reward:  0
line 245: next_state:  tensor([[0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
         0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.,
         1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.]])
Episode:  9
t:  0
action:  tensor([[8]])
reward:  0
line 245: next_state:  tensor([[0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0.,
         0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.,
         1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.]])
Episode:  10
t:  0
action:  tensor([[15]])
reward:  -100
line 242 done
2023-01-31 15:26:25.181 Python[14992:195228] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/6n/f9xnq2pn0c372p4ksdc50x5r0000gn/T/org.python.python.savedState
Episode:  11
t:  0
action:  tensor([[8]])
reward:  -100
line 242 done
Episode:  12
t:  0
action:  tensor([[52]])
reward:  -100
line 242 done
Episode:  13
t:  0
action:  tensor([[55]])
reward:  -100
line 242 done
Episode:  14
t:  0
action:  tensor([[55]])
reward:  -100
line 242 done
Episode:  15
t:  0
action:  tensor([[43]])
reward:  -100
line 242 done
Episode:  16
t:  0
action:  tensor([[53]])
reward:  -100
line 242 done
Episode:  17
t:  0
action:  tensor([[56]])
reward:  -100
line 242 done
Episode:  18
t:  0
action:  tensor([[27]])
reward:  -100
line 242 done
Episode:  19
t:  0
action:  tensor([[61]])
reward:  -100
line 242 done
Episode:  20
t:  0
action:  tensor([[29]])
reward:  -100
line 242 done
Episode:  21
t:  0
action:  tensor([[40]])
reward:  -100
line 242 done
Episode:  22
t:  0
action:  tensor([[13]])
reward:  -100
line 242 done
Episode:  23
t:  0
action:  tensor([[64]])
reward:  -100
line 242 done
Episode:  24
t:  0
action:  tensor([[62]])
reward:  -100
line 242 done
Episode:  25
t:  0
action:  tensor([[39]])
reward:  -100
line 242 done
Episode:  26
t:  0
action:  tensor([[65]])
reward:  -100
line 242 done
Episode:  27
t:  0
action:  tensor([[20]])
reward:  -100
line 242 done
Episode:  28
t:  0
action:  tensor([[47]])
reward:  -100
line 242 done
Episode:  29
t:  0
action:  tensor([[15]])
reward:  -100
line 242 done
Episode:  30
t:  0
action:  tensor([[47]])
reward:  -100
line 242 done
Episode:  31
t:  0
action:  tensor([[13]])
reward:  -100
line 242 done
Episode:  32
t:  0
action:  tensor([[40]])
reward:  -100
line 242 done
Episode:  33
t:  0
action:  tensor([[48]])
reward:  -100
line 242 done
Episode:  34
t:  0
action:  tensor([[65]])
reward:  -100
line 242 done
Episode:  35
t:  0
action:  tensor([[62]])
reward:  -100
line 242 done
Episode:  36
t:  0
action:  tensor([[50]])
reward:  -100
line 242 done
Episode:  37
t:  0
action:  tensor([[62]])
reward:  -100
line 242 done
Episode:  38
t:  0
action:  tensor([[49]])
reward:  -100
line 242 done
Episode:  39
t:  0
action:  tensor([[67]])
reward:  -100
line 242 done
Episode:  40
t:  0
action:  tensor([[42]])
reward:  -100
line 242 done
Episode:  41
t:  0
action:  tensor([[49]])
reward:  -100
line 242 done
Episode:  42
t:  0
action:  tensor([[28]])
reward:  -100
line 242 done
Episode:  43
t:  0
action:  tensor([[22]])
reward:  -100
line 242 done
Episode:  44
t:  0
action:  tensor([[58]])
reward:  -100
line 242 done
Episode:  45
t:  0
action:  tensor([[1]])
reward:  -100
line 242 done
Episode:  46
t:  0
action:  tensor([[33]])
reward:  -100
line 242 done
Episode:  47
t:  0
action:  tensor([[37]])
reward:  -100
line 242 done
Episode:  48
t:  0
action:  tensor([[28]])
reward:  -100
line 242 done
Episode:  49
t:  0
action:  tensor([[28]])
reward:  -100
line 242 done
Complete
2023-01-31 15:26:36.857 Python[14992:195228] +[CATransaction synchronize] called within transaction
